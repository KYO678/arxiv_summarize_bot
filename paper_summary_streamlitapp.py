import streamlit as st
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import openai
from notion_client import Client
import re
import time
import random
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import pickle
import os

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ“š Paper Summary by LLM",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS - ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ & å¤§äººã£ã½ã„é…è‰²
st.markdown("""
<style>
    /* å…¨ä½“ã®ãƒ™ãƒ¼ã‚¹è¨­å®š */
    .stApp {
        background-color: #262236;
        color: #fefef3;
    }
    
    /* ã‚µã‚¤ãƒ‰ãƒãƒ¼ */
    .css-1d391kg {
        background-color: #3d4f7e;
    }
    
    /* ãƒ¡ã‚¤ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼ */
    .main-header {
        text-align: center;
        padding: 3rem 0;
        background: linear-gradient(135deg, #3d4f7e 0%, #262236 50%, #e18546 100%);
        color: #fefef3;
        margin: -1rem -1rem 2rem -1rem;
        border-radius: 0 0 20px 20px;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        position: relative;
        overflow: hidden;
    }
    
    .main-header::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        bottom: 0;
        background: radial-gradient(circle at 30% 40%, rgba(225, 133, 70, 0.1) 0%, transparent 50%);
        pointer-events: none;
    }
    
    .main-header h1 {
        font-size: 2.8rem;
        margin-bottom: 0.5rem;
        font-weight: 700;
        text-shadow: 0 2px 10px rgba(0, 0, 0, 0.3);
    }
    
    .main-header p {
        font-size: 1.3rem;
        opacity: 0.9;
        font-weight: 300;
    }
    
    /* æ¤œç´¢æ–¹æ³•ã‚³ãƒ³ãƒ†ãƒŠ */
    .search-method-container {
        background: linear-gradient(135deg, #3d4f7e 0%, #495a8a 100%);
        padding: 1.5rem;
        border-radius: 15px;
        border: 1px solid #4a5c91;
        margin-bottom: 1.5rem;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.2);
    }
    
    /* è«–æ–‡æƒ…å ±ãƒœãƒƒã‚¯ã‚¹ */
    .paper-info-box {
        background: linear-gradient(135deg, #3d4f7e 0%, #495a8a 100%);
        border: 2px solid #e18546;
        border-radius: 15px;
        padding: 2rem;
        margin: 1.5rem 0;
        box-shadow: 0 8px 32px rgba(225, 133, 70, 0.1);
        position: relative;
        overflow: hidden;
    }
    
    .paper-info-box::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: 4px;
        background: linear-gradient(90deg, #e18546 0%, #f4a261 100%);
    }
    
    /* è¦ç´„ãƒœãƒƒã‚¯ã‚¹ */
    .summary-box {
        background: linear-gradient(135deg, #2a1f3d 0%, #3d4f7e 100%);
        border-left: 6px solid #e18546;
        border-radius: 0 15px 15px 0;
        padding: 2rem;
        margin: 1.5rem 0;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
        position: relative;
    }
    
    .summary-box::after {
        content: "";
        position: absolute;
        right: 20px;
        top: 20px;
        width: 40px;
        height: 40px;
        background: radial-gradient(circle, #e18546 0%, transparent 70%);
        border-radius: 50%;
        opacity: 0.3;
    }
    
    /* ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒœãƒƒã‚¯ã‚¹ */
    .success-message {
        background: linear-gradient(135deg, #2d5016 0%, #52b788 100%);
        color: #fefef3;
        border: 1px solid #52b788;
        border-radius: 12px;
        padding: 1.2rem;
        margin: 1rem 0;
        box-shadow: 0 4px 16px rgba(82, 183, 136, 0.2);
    }
    
    .error-message {
        background: linear-gradient(135deg, #8b2635 0%, #e63946 100%);
        color: #fefef3;
        border: 1px solid #e63946;
        border-radius: 12px;
        padding: 1.2rem;
        margin: 1rem 0;
        box-shadow: 0 4px 16px rgba(230, 57, 70, 0.2);
    }
    
    .warning-message {
        background: linear-gradient(135deg, #b5651d 0%, #e18546 100%);
        color: #fefef3;
        border: 1px solid #e18546;
        border-radius: 12px;
        padding: 1.2rem;
        margin: 1rem 0;
        box-shadow: 0 4px 16px rgba(225, 133, 70, 0.2);
    }
    
    /* ãƒ•ãƒƒã‚¿ãƒ¼ */
    .footer-tips {
        background: linear-gradient(135deg, #2a1f3d 0%, #3d4f7e 100%);
        border-radius: 15px;
        padding: 2rem;
        margin-top: 2rem;
        border: 1px solid #4a5c91;
        box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
    }
    
    /* ãƒœã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ« */
    .stButton > button {
        width: 100%;
        border-radius: 12px;
        font-weight: 600;
        transition: all 0.3s ease;
        background: linear-gradient(135deg, #e18546 0%, #f4a261 100%);
        border: none;
        color: #262236;
        font-size: 1rem;
        padding: 0.6rem 1.2rem;
        box-shadow: 0 4px 16px rgba(225, 133, 70, 0.3);
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 24px rgba(225, 133, 70, 0.4);
        background: linear-gradient(135deg, #f4a261 0%, #e76f51 100%);
    }
    
    .stButton > button:active {
        transform: translateY(0px);
        box-shadow: 0 4px 16px rgba(225, 133, 70, 0.3);
    }
    
    /* ãƒ—ãƒ©ã‚¤ãƒãƒªãƒœã‚¿ãƒ³ */
    .stButton > button[kind="primary"] {
        background: linear-gradient(135deg, #3d4f7e 0%, #495a8a 100%);
        color: #fefef3;
        box-shadow: 0 4px 16px rgba(61, 79, 126, 0.3);
    }
    
    .stButton > button[kind="primary"]:hover {
        background: linear-gradient(135deg, #495a8a 0%, #5a6ba3 100%);
        box-shadow: 0 8px 24px rgba(61, 79, 126, 0.4);
    }
    
    /* å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ */
    .stTextInput > div > div > input {
        background-color: #3d4f7e;
        color: #fefef3;
        border: 2px solid #4a5c91;
        border-radius: 10px;
        padding: 0.7rem;
    }
    
    .stTextInput > div > div > input:focus {
        border-color: #e18546;
        box-shadow: 0 0 10px rgba(225, 133, 70, 0.3);
    }
    
    /* ã‚»ãƒ¬ã‚¯ãƒˆãƒœãƒƒã‚¯ã‚¹ */
    .stSelectbox > div > div > select {
        background-color: #3d4f7e;
        color: #fefef3;
        border: 2px solid #4a5c91;
        border-radius: 10px;
    }
    
    /* ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒªã‚¢ */
    .stTextArea > div > div > textarea {
        background-color: #3d4f7e;
        color: #fefef3;
        border: 2px solid #4a5c91;
        border-radius: 10px;
    }
    
    .stTextArea > div > div > textarea:focus {
        border-color: #e18546;
        box-shadow: 0 0 10px rgba(225, 133, 70, 0.3);
    }
    
    /* ãƒ©ã‚¸ã‚ªãƒœã‚¿ãƒ³ */
    .stRadio > div {
        background-color: rgba(61, 79, 126, 0.3);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #4a5c91;
    }
    
    /* ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ */
    .streamlit-expanderHeader {
        background-color: #3d4f7e;
        color: #fefef3;
        border-radius: 10px;
        border: 1px solid #4a5c91;
    }
    
    .streamlit-expanderContent {
        background-color: rgba(61, 79, 126, 0.2);
        border: 1px solid #4a5c91;
        border-top: none;
        border-radius: 0 0 10px 10px;
    }
    
    /* ãƒ¡ãƒˆãƒªã‚¯ã‚¹ */
    .metric-container {
        background: linear-gradient(135deg, #3d4f7e 0%, #495a8a 100%);
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #4a5c91;
        margin: 0.5rem 0;
    }
    
    /* ã‚¹ãƒ”ãƒŠãƒ¼ */
    .stSpinner {
        color: #e18546 !important;
    }
    
    /* ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ */
    .stMarkdown code {
        background-color: #2a1f3d;
        color: #e18546;
        padding: 0.2rem 0.4rem;
        border-radius: 4px;
        border: 1px solid #4a5c91;
    }
    
    /* ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ */
    .stProgress > div > div > div {
        background-color: #e18546;
    }
    
    /* æƒ…å ±ãƒœãƒƒã‚¯ã‚¹ */
    .stInfo {
        background-color: rgba(61, 79, 126, 0.3);
        border-left: 4px solid #e18546;
        color: #fefef3;
    }
    
    /* åˆ—ã®åŒºåˆ‡ã‚Šç·š */
    .element-container {
        border-right: 1px solid rgba(254, 254, 243, 0.1);
    }
    
    /* ã‚«ã‚¹ã‚¿ãƒ ã‚¢ã‚¯ã‚»ãƒ³ãƒˆ */
    .accent-gradient {
        background: linear-gradient(45deg, #e18546, #3d4f7e);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
        font-weight: bold;
    }
    
    /* ãƒ›ãƒãƒ¼ã‚¨ãƒ•ã‚§ã‚¯ãƒˆ */
    .hover-glow:hover {
        box-shadow: 0 0 20px rgba(225, 133, 70, 0.3);
        transition: all 0.3s ease;
    }
    
    /* ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ãƒãƒ¼ */
    ::-webkit-scrollbar {
        width: 8px;
    }
    
    ::-webkit-scrollbar-track {
        background: #262236;
    }
    
    ::-webkit-scrollbar-thumb {
        background: #e18546;
        border-radius: 4px;
    }
    
    ::-webkit-scrollbar-thumb:hover {
        background: #f4a261;
    }
</style>
""", unsafe_allow_html=True)

# å®šæ•°
SLACK_CHANNEL = "#news-bot1"
CACHE_DIR = "arxiv_cache"
ARXIV_API_BASE = "http://export.arxiv.org/api/query"

GPT_MODELS = {
    "GPT-4o": "gpt-4o-2024-08-06",
    "GPT-4.1 nano": "gpt-4.1-nano-2025-04-14", 
    "GPT-4.1": "gpt-4.1-2025-04-14",
    "o3": "o3-2025-04-16"
}

DEFAULT_PROMPT = """ã¾ãšã€ä¸ãˆã‚‰ã‚ŒãŸè«–æ–‡ã®èƒŒæ™¯ã¨ãªã£ã¦ã„ãŸèª²é¡Œã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚
æ¬¡ã«ã€è¦ç‚¹ã‚’3ç‚¹ã€ã¾ã¨ã‚ã¦ä¸‹ã•ã„ã€‚
æ›´ã«ã€ä»Šå¾Œã®å±•æœ›ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
æœ€å¾Œã«ã€ä¸ãˆã‚‰ã‚ŒãŸè«–æ–‡ã«ã¤ã„ã¦æƒ³å®šã•ã‚Œå¾—ã‚‹æ‰¹åˆ¤ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‚‰ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
```
ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã®æ—¥æœ¬èªè¨³
ãƒ»èƒŒæ™¯èª²é¡Œ
ãƒ»è¦ç‚¹1
ãƒ»è¦ç‚¹2
ãƒ»è¦ç‚¹3
ãƒ»ä»Šå¾Œã®å±•æœ›
ãƒ»æƒ³å®šã•ã‚Œã‚‹æ‰¹åˆ¤
```
"""

def load_config():
    """Streamlit Secretsã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
    try:
        # Streamlit Secretsã‹ã‚‰èª­ã¿è¾¼ã¿
        config = {
            'api_keys': {
                'openai': st.secrets["gptApiKey"]["key"],
                'slack': st.secrets["SlackApiKey"]["key"],
                'notion': st.secrets["NotionApiKey"]["key"]
            },
            'settings': {
                'notion_database_url': st.secrets["NotionDatabaseUrl"]["key"],
                'slack_channel': SLACK_CHANNEL
            }
        }
        
        # å¿…è¦ãªã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèª
        for key, value in config['api_keys'].items():
            if not value:
                st.error(f"âŒ {key} APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
                st.stop()
        
        return config
        
    except Exception as e:
        st.error(f"""
        âŒ ã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆè¨­å®šã‚¨ãƒ©ãƒ¼: {e}
        
        GitHub Secretsã¾ãŸã¯.streamlit/secrets.tomlã«ä»¥ä¸‹ã®å½¢å¼ã§è¨­å®šã—ã¦ãã ã•ã„ï¼š
        
        ```toml
        [gptApiKey]
        key = "your-openai-api-key"
        
        [SlackApiKey]
        key = "your-slack-bot-token"
        
        [NotionApiKey]
        key = "your-notion-api-key"
        
        [NotionDatabaseUrl]
        key = "your-notion-database-id"
        ```
        """)
        st.stop()

# æ”¹è‰¯ã•ã‚ŒãŸarXivæ¤œç´¢ã‚¯ãƒ©ã‚¹
class ImprovedArxivSearch:
    def __init__(self, cache_dir=CACHE_DIR):
        self.api_base = ARXIV_API_BASE
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def extract_arxiv_id_from_url(self, url):
        """arXiv URLã‹ã‚‰IDã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            patterns = [
                r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5}v?[0-9]*)',
                r'arxiv\.org/pdf/([0-9]{4}\.[0-9]{4,5}v?[0-9]*)\.pdf',
                r'^([0-9]{4}\.[0-9]{4,5}v?[0-9]*)$',
                r'arxiv\.org/abs/([a-z-]+/[0-9]{7})',  # å¤ã„å½¢å¼
                r'arxiv\.org/pdf/([a-z-]+/[0-9]{7})'   # å¤ã„å½¢å¼
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    arxiv_id = match.group(1)
                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å‰Šé™¤
                    arxiv_id = re.sub(r'v\d+$', '', arxiv_id)
                    return arxiv_id
            
            return None
        except Exception:
            return None
    
    def get_cached_results(self, query, max_age_hours=6):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—"""
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        
        if os.path.exists(cache_path):
            mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))
            if datetime.now() - mtime < timedelta(hours=max_age_hours):
                try:
                    with open(cache_path, 'rb') as f:
                        return pickle.load(f)
                except Exception:
                    os.remove(cache_path)
        
        return None
    
    def save_results(self, query, results):
        """çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(results, f)
        except Exception as e:
            st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã«å¤±æ•—: {e}")
    
    def search_arxiv_api(self, query, max_results=5, retry_count=3):
        """ç›´æ¥arXiv APIã§æ¤œç´¢"""
        params = {
            'search_query': query,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        for attempt in range(retry_count):
            try:
                response = requests.get(self.api_base, params=params, timeout=30)
                response.raise_for_status()
                
                # XMLãƒ‘ãƒ¼ã‚¹
                root = ET.fromstring(response.content)
                
                # åå‰ç©ºé–“ã®è¨­å®š
                namespaces = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                entries = root.findall('atom:entry', namespaces)
                if not entries:
                    return None
                
                # æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—
                entry = entries[0]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
                paper_data = {
                    'id': entry.find('atom:id', namespaces).text.split('/')[-1],
                    'title': entry.find('atom:title', namespaces).text.strip(),
                    'summary': entry.find('atom:summary', namespaces).text.strip(),
                    'published': entry.find('atom:published', namespaces).text,
                    'updated': entry.find('atom:updated', namespaces).text,
                    'authors': [author.find('atom:name', namespaces).text 
                               for author in entry.findall('atom:author', namespaces)],
                    'categories': [cat.get('term') 
                                  for cat in entry.findall('atom:category', namespaces)]
                }
                
                # URLæƒ…å ±ã‚’è¿½åŠ 
                paper_data['entry_id'] = f"http://arxiv.org/abs/{paper_data['id']}"
                paper_data['pdf_url'] = f"http://arxiv.org/pdf/{paper_data['id']}.pdf"
                
                # æ—¥ä»˜æƒ…å ±ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                try:
                    paper_data['published_datetime'] = datetime.fromisoformat(paper_data['published'].replace('Z', '+00:00'))
                except:
                    paper_data['published_datetime'] = datetime.now()
                
                return paper_data
                
            except requests.RequestException as e:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                if attempt < retry_count - 1:
                    st.warning(f"âš ï¸ APIæ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼ˆ{attempt + 1}/{retry_count}ï¼‰ã€‚{wait_time:.1f}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(wait_time)
                else:
                    st.error(f"âŒ APIæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    
            except ET.ParseError as e:
                st.error(f"âŒ XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        return None
    
    def search_by_id(self, arxiv_id):
        """IDã§è«–æ–‡ã‚’æ¤œç´¢"""
        cache_key = f"id:{arxiv_id}"
        cached_result = self.get_cached_results(cache_key)
        if cached_result:
            st.info("ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
            return cached_result
        
        # IDæ¤œç´¢ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            'id_list': arxiv_id,
            'max_results': 1
        }
        
        try:
            response = requests.get(self.api_base, params=params, timeout=30)
            response.raise_for_status()
            
            # XMLãƒ‘ãƒ¼ã‚¹
            root = ET.fromstring(response.content)
            
            # åå‰ç©ºé–“ã®è¨­å®š
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entry = root.find('atom:entry', namespaces)
            if entry is None:
                return None
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            paper_data = {
                'id': entry.find('atom:id', namespaces).text.split('/')[-1],
                'title': entry.find('atom:title', namespaces).text.strip(),
                'summary': entry.find('atom:summary', namespaces).text.strip(),
                'published': entry.find('atom:published', namespaces).text,
                'updated': entry.find('atom:updated', namespaces).text,
                'authors': [author.find('atom:name', namespaces).text 
                           for author in entry.findall('atom:author', namespaces)],
                'categories': [cat.get('term') 
                              for cat in entry.findall('atom:category', namespaces)]
            }
            
            # URLæƒ…å ±ã‚’è¿½åŠ 
            paper_data['entry_id'] = f"http://arxiv.org/abs/{paper_data['id']}"
            paper_data['pdf_url'] = f"http://arxiv.org/pdf/{paper_data['id']}.pdf"
            
            # æ—¥ä»˜æƒ…å ±ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            try:
                paper_data['published_datetime'] = datetime.fromisoformat(paper_data['published'].replace('Z', '+00:00'))
            except:
                paper_data['published_datetime'] = datetime.now()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.save_results(cache_key, paper_data)
            
            return paper_data
            
        except Exception as e:
            st.error(f"âŒ IDæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def search_by_title(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ã§è«–æ–‡ã‚’æ¤œç´¢"""
        cache_key = f"title:{title}"
        cached_result = self.get_cached_results(cache_key)
        if cached_result:
            st.info("ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
            return cached_result
        
        # ã¾ãšå®Œå…¨ä¸€è‡´æ¤œç´¢ã‚’è©¦è¡Œ
        query = f'ti:"{title}"'
        result = self.search_arxiv_api(query, max_results=3)
        
        if result:
            self.save_results(cache_key, result)
            return result
        
        # å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
        st.info("ğŸ” å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        result = self.search_arxiv_api(f"all:{title}", max_results=5)
        
        if result:
            self.save_results(cache_key, result)
            return result
        
        return None

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹
class ArxivCache:
    def __init__(self, cache_dir=CACHE_DIR):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cached_results(self, query, max_age_hours=24):
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        
        if os.path.exists(cache_path):
            mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))
            if datetime.now() - mtime < timedelta(hours=max_age_hours):
                try:
                    with open(cache_path, 'rb') as f:
                        return pickle.load(f)
                except Exception:
                    os.remove(cache_path)
        
        return None
    
    def save_results(self, query, results):
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(results, f)
        except Exception as e:
            st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã«å¤±æ•—: {e}")

# APIè¨­å®šã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
@st.cache_resource
def initialize_apis():
    """APIåˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    config = load_config()
    
    try:
        # API ã‚­ãƒ¼ã®å–å¾—
        openai_key = config['api_keys']['openai']
        slack_token = config['api_keys']['slack']
        notion_key = config['api_keys']['notion']
        notion_db_url = config['settings']['notion_database_url']
        
        # OpenAIåˆæœŸåŒ–
        openai.api_key = openai_key
        
        # NotionåˆæœŸåŒ–
        notion_client = Client(auth=notion_key)
        
        # SlackåˆæœŸåŒ–
        slack_client = WebClient(token=slack_token)
        
        # æ”¹è‰¯ã•ã‚ŒãŸarXivæ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        arxiv_search = ImprovedArxivSearch()
        
        return {
            "openai_key": openai_key,
            "slack_client": slack_client,
            "notion_client": notion_client,
            "notion_db_url": notion_db_url,
            "arxiv_search": arxiv_search,
            "cache": ArxivCache(),
            "config": config
        }
    except Exception as e:
        st.error(f"âš ï¸ APIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        st.stop()

def search_paper_by_title(title, apis):
    """ã‚¿ã‚¤ãƒˆãƒ«ã§è«–æ–‡ã‚’æ¤œç´¢"""
    try:
        result = apis["arxiv_search"].search_by_title(title)
        return result
    except Exception as e:
        st.error(f"âŒ è«–æ–‡æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def search_paper_by_id(arxiv_id, apis):
    """arXiv IDã§è«–æ–‡ã‚’æ¤œç´¢"""
    try:
        result = apis["arxiv_search"].search_by_id(arxiv_id)
        return result
    except Exception as e:
        st.error(f"âŒ è«–æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_summary(prompt, result, model, apis):
    """è«–æ–‡è¦ç´„ã‚’ç”Ÿæˆï¼ˆä¿®æ­£ç‰ˆï¼‰"""
    if not prompt.strip():
        st.error("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã§ã™ã€‚")
        return None
        
    text = f"title: {result['title']}\nbody: {result['summary']}"
    
    try:
        # å¤ã„APIå½¢å¼ã‚’å…ˆã«è©¦ã™
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": text},
            ],
            temperature=0.25,
        )
        summary = response["choices"][0]["message"]["content"]
        
    except Exception as e:
        # æ–°ã—ã„APIå½¢å¼ã§ãƒªãƒˆãƒ©ã‚¤
        try:
            client = openai.OpenAI(api_key=apis["openai_key"])
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.25,
                max_tokens=2000,
            )
            summary = response.choices[0].message.content
        except Exception as e2:
            st.error(f"âŒ OpenAI APIã‚¨ãƒ©ãƒ¼: {e2}")
            return None
    
    if not summary:
        st.error("âŒ è¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    # ã‚µãƒãƒªãƒ¼ã®ã¿ã‚’è¿”ã™ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã¯å¾Œã§è¿½åŠ ï¼‰
    return summary

def add_summary_to_notion(summary_data, apis):
    """Notionã«è¦ç´„ã‚’è¿½åŠ ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ã®ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    try:
        # Database IDã‚’ãã®ã¾ã¾ä½¿ç”¨
        notion_db_id = apis["notion_db_url"]
        
        # ãƒšãƒ¼ã‚¸ä½œæˆãƒ‡ãƒ¼ã‚¿
        result = apis["notion_client"].pages.create(
            parent={"database_id": notion_db_id},
            properties={
                "Name": {
                    "title": [
                        {
                            "text": {
                                "content": summary_data["title"]
                            }
                        }
                    ]
                },
                "Tags": {
                    "multi_select": [
                        {
                            "name": "arXiv"
                        }
                    ]
                },
                "Published": {
                    "date": {
                        "start": summary_data["date"]
                    }
                },
                "URL": {
                    "url": summary_data["url"]
                }
            },
            children=[
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [
                            {
                                "type": "text",
                                "text": {
                                    "content": summary_data["summary"][:2000]  # Notionåˆ¶é™
                                }
                            }
                        ]
                    }
                }
            ]
        )
        
        return True, "Notionã«ä¿å­˜ã•ã‚Œã¾ã—ãŸ"
        
    except Exception as e:
        return False, f"Notion API ã‚¨ãƒ©ãƒ¼: {str(e)}"

def post_to_slack(message, apis):
    """Slackã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿ï¼ˆå‹•ä½œç¢ºèªæ¸ˆã¿ã®ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆï¼‰"""
    try:
        # ãƒãƒ£ãƒ³ãƒãƒ«åã‚’å–å¾—
        channel = apis["config"]["settings"].get("slack_channel", SLACK_CHANNEL)
        
        # SlackæŠ•ç¨¿
        response = apis["slack_client"].chat_postMessage(
            channel=channel,
            text=message
        )
        
        if response["ok"]:
            return True, "Slackã«æŠ•ç¨¿ã•ã‚Œã¾ã—ãŸ"
        else:
            return False, f"Slack ã‚¨ãƒ©ãƒ¼: {response.get('error', 'Unknown')}"
            
    except SlackApiError as e:
        return False, f"Slack API ã‚¨ãƒ©ãƒ¼: {e.response['error']}"
    except Exception as e:
        return False, f"SlackæŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: {str(e)}"

def display_paper_info(result):
    """è«–æ–‡æƒ…å ±ã‚’è¡¨ç¤º"""
    st.markdown('<div class="paper-info-box">', unsafe_allow_html=True)
    st.markdown("### ğŸ“„ è«–æ–‡æƒ…å ±")
    
    st.write(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {result['title']}")
    st.write(f"**ç™ºè¡Œæ—¥:** {result['published_datetime'].strftime('%Y-%m-%d')}")
    st.write(f"**URL:** {result['entry_id']}")
    
    try:
        if result['authors']:
            displayed_authors = result['authors'][:10]
            author_text = ", ".join(displayed_authors)
            if len(result['authors']) > 10:
                author_text += f" ä»– {len(result['authors']) - 10} å"
            st.write(f"**è‘—è€…:** {author_text}")
    except Exception:
        st.write("**è‘—è€…:** æƒ…å ±å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    if result.get('categories'):
        st.write(f"**ã‚«ãƒ†ã‚´ãƒª:** {', '.join(result['categories'][:5])}")
    
    st.markdown('</div>', unsafe_allow_html=True)

def main():
    # åˆæœŸåŒ–
    apis = initialize_apis()
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ“š Paper Summary by ChatGPT</h1>
        <p>arXivã®è«–æ–‡ã‚’æ¤œç´¢ã—ã¦AIã§è¦ç´„ã™ã‚‹ã‚¢ãƒ—ãƒªã§ã™</p>
    </div>
    """, unsafe_allow_html=True)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # GPTãƒ¢ãƒ‡ãƒ«é¸æŠ
        selected_model_name = st.selectbox(
            "GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            options=list(GPT_MODELS.keys()),
            index=0
        )
        selected_model = GPT_MODELS[selected_model_name]
        
        st.info(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: **{selected_model_name}**")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š çµ±è¨ˆæƒ…å ±")
        if 'search_count' not in st.session_state:
            st.session_state.search_count = 0
        if 'error_count' not in st.session_state:
            st.session_state.error_count = 0
        if 'cache_hits' not in st.session_state:
            st.session_state.cache_hits = 0
            
        st.metric("æ¤œç´¢å›æ•°", st.session_state.search_count)
        st.metric("ã‚¨ãƒ©ãƒ¼å›æ•°", st.session_state.error_count)
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ", st.session_state.cache_hits)

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # æ¤œç´¢æ–¹æ³•é¸æŠ
        st.markdown('<div class="search-method-container">', unsafe_allow_html=True)
        search_method = st.radio(
            "è«–æ–‡ã®æŒ‡å®šæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            ["ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢", "URLã¾ãŸã¯IDã§æŒ‡å®š"],
            horizontal=True
        )
        st.markdown('</div>', unsafe_allow_html=True)

        # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if search_method == "ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢":
            paper_input = st.text_input(
                "ğŸ“ arXivã®è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                placeholder="ä¾‹: Attention Is All You Need",
                help="è«–æ–‡ã®æ­£ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚éƒ¨åˆ†ä¸€è‡´ã§ã‚‚æ¤œç´¢ã§ãã¾ã™ã€‚"
            )
        else:
            paper_input = st.text_input(
                "ğŸ”— arXivã®URLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                placeholder="ä¾‹: https://arxiv.org/abs/1706.03762 ã¾ãŸã¯ 1706.03762",
                help="arXivã®URLã€PDFãƒªãƒ³ã‚¯ã€ã¾ãŸã¯IDï¼ˆ1234.5678å½¢å¼ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
            )

    with col2:
        st.markdown("### ğŸ¯ ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹")
        st.markdown('<span class="accent-gradient">**äººæ°—ã®è«–æ–‡ä¾‹:**</span>', unsafe_allow_html=True)
        
        example_papers = [
            ("Attention Is All You Need", "1706.03762"),
            ("BERT", "1810.04805"),
            ("GPT-3", "2005.14165")
        ]
        
        for title, paper_id in example_papers:
            if st.button(f"ğŸ“„ {title}", key=f"example_{paper_id}"):
                st.session_state.paper_input = paper_id
                st.rerun()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    with st.expander("ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º", expanded=False):
        custom_prompt = st.text_area(
            "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:",
            value=DEFAULT_PROMPT,
            height=300,
            help="è«–æ–‡è¦ç´„ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªç”±ã«ç·¨é›†ã§ãã¾ã™"
        )
    
    # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãŒé–‰ã˜ã¦ã„ã‚‹å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    if 'custom_prompt' not in locals():
        custom_prompt = DEFAULT_PROMPT

    # æ¤œç´¢ãƒœã‚¿ãƒ³
    search_clicked = st.button(
        "ğŸ” è«–æ–‡ã‚’æ¤œç´¢ã—ã¦è¦ç´„", 
        type="primary",
        use_container_width=True
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ã®å…¥åŠ›å¾©å…ƒ
    if 'paper_input' in st.session_state:
        paper_input = st.session_state.paper_input
        del st.session_state.paper_input

    # æ¤œç´¢å®Ÿè¡Œ
    if search_clicked:
        if not paper_input.strip():
            st.error("âŒ è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # æ¤œç´¢ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
        st.session_state.search_count += 1

        # è«–æ–‡æ¤œç´¢
        with st.spinner("ğŸ” è«–æ–‡ã‚’æ¤œç´¢ä¸­..."):
            if search_method == "ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢":
                result = search_paper_by_title(paper_input.strip(), apis)
            else:
                arxiv_id = apis["arxiv_search"].extract_arxiv_id_from_url(paper_input.strip())
                if not arxiv_id:
                    st.error("âŒ æœ‰åŠ¹ãªarXiv URLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    st.session_state.error_count += 1
                    return
                result = search_paper_by_id(arxiv_id, apis)
            
            if not result:
                st.error("âŒ è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…¥åŠ›å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.session_state.error_count += 1
                return

        # è«–æ–‡æƒ…å ±è¡¨ç¤º
        st.markdown('<div class="success-message">âœ… è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼</div>', unsafe_allow_html=True)
        display_paper_info(result)

        # è¦ç´„ç”Ÿæˆ
        with st.spinner(f"ğŸ¤– {selected_model_name}ã§è¦ç´„ä¸­..."):
            summary_text = get_summary(custom_prompt, result, selected_model, apis)
            
            if not summary_text:
                st.error("âŒ è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.session_state.error_count += 1
                return

            # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ä½œæˆ
            date_str = result['published_datetime'].strftime("%Y-%m-%d %H:%M:%S")
            summary_message = f"ç™ºè¡Œæ—¥: {date_str}\n{result['entry_id']}\n{result['title']}\n\n{summary_text}\n"
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜ï¼ˆãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã‚‚æ¶ˆãˆãªã„ã‚ˆã†ã«ï¼‰
            st.session_state.last_result = {
                "paper_info": result,
                "summary_text": summary_text,
                "summary_message": summary_message,
                "summary_data": {
                    "title": result['title'],
                    "summary": summary_text,  # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“ã§ã¯ãªãè¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã®ã¿
                    "url": result['entry_id'],
                    "date": result['published_datetime'].strftime("%Y-%m-%d"),
                }
            }

    # çµæœè¡¨ç¤ºï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ï¼‰
    if 'last_result' in st.session_state:
        result_data = st.session_state.last_result
        
        # è¦ç´„çµæœè¡¨ç¤º
        st.markdown("## ğŸ“‹ è¦ç´„çµæœ")
        st.markdown('<div class="summary-box">', unsafe_allow_html=True)
        st.markdown(result_data["summary_message"])
        st.markdown('</div>', unsafe_allow_html=True)

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³
        st.markdown("### ğŸ“¤ å…±æœ‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³")
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“¢ Slackã«æŠ•ç¨¿", use_container_width=True, key="slack_post"):
                with st.spinner("Slackã«æŠ•ç¨¿ä¸­..."):
                    message = "è«–æ–‡ã®ã‚µãƒãƒªã§ã™ã€‚\n" + result_data["summary_message"]
                    success, msg = post_to_slack(message, apis)
                    if success:
                        st.success(f"âœ… {msg}")
                    else:
                        st.error(f"âŒ {msg}")

        with col2:
            if st.button("ğŸ“ Notionã«ä¿å­˜", use_container_width=True, key="notion_save"):
                with st.spinner("Notionã«ä¿å­˜ä¸­..."):
                    success, msg = add_summary_to_notion(result_data["summary_data"], apis)
                    if success:
                        st.success(f"âœ… {msg}")
                    else:
                        st.error(f"âŒ {msg}")

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown('<div class="footer-tips">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¡ ä½¿ã„æ–¹ã®ãƒ’ãƒ³ãƒˆ")
    st.markdown("""
    - **ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢**: è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚‚å¯èƒ½ï¼‰
    - **URL/IDæŒ‡å®š**: `https://arxiv.org/abs/1234.5678` å½¢å¼ã®URLã¾ãŸã¯ `1234.5678` å½¢å¼ã®IDãŒä½¿ç”¨ã§ãã¾ã™
    - **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: è¦ç´„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã€ã‹ã‚‰ç·¨é›†ã—ã¦ãã ã•ã„
    - **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ç”¨é€”ã«å¿œã˜ã¦GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„
    """)
    
    st.markdown("### ğŸ¨ ã“ã®ã‚¢ãƒ—ãƒªã«ã¤ã„ã¦")
    st.markdown("""
    - **<span class="accent-gradient">æ´—ç·´ã•ã‚ŒãŸãƒ‡ã‚¶ã‚¤ãƒ³</span>**: ãƒ€ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ç¾ã—ã„UI
    - **é«˜é€Ÿæ¤œç´¢**: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ã«ã‚ˆã‚ŠåŒã˜æ¤œç´¢ã¯é«˜é€Ÿè¡¨ç¤º
    - **APIé€£æº**: Slackã€Notionã¸ã®è‡ªå‹•æŠ•ç¨¿æ©Ÿèƒ½
    - **ã‚¨ãƒ©ãƒ¼è€æ€§**: è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    """, unsafe_allow_html=True)
    
    st.markdown('</div>', unsafe_allow_html=True)

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†
if __name__ == '__main__':
    main()
