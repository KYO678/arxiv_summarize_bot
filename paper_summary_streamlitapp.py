import streamlit as st
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import arxiv
import openai
from notion_client import Client
import re
import time
import random
import requests
from datetime import datetime, timedelta
import pickle
import os

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ“š Paper Summary by ChatGPT",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin: -1rem -1rem 2rem -1rem;
        border-radius: 0 0 15px 15px;
    }
    .main-header h1 {
        font-size: 2.5rem;
        margin-bottom: 0.5rem;
    }
    .main-header p {
        font-size: 1.2rem;
        opacity: 0.9;
    }
    .search-method-container {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #e9ecef;
        margin-bottom: 1rem;
    }
    .paper-info-box {
        background: #e3f2fd;
        border: 1px solid #2196f3;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    .summary-box {
        background: #f8f9fa;
        border-left: 5px solid #667eea;
        padding: 1.5rem;
        margin: 1rem 0;
        border-radius: 0 10px 10px 0;
    }
    .success-message {
        background: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-message {
        background: #f8d7da;
        color: #721c24;
        border: 1px solid #f5c6cb;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-message {
        background: #fff3cd;
        color: #856404;
        border: 1px solid #ffeaa7;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .footer-tips {
        background: #f8f9fa;
        border-radius: 10px;
        padding: 1.5rem;
        margin-top: 2rem;
        border: 1px solid #e9ecef;
    }
    .stButton > button {
        width: 100%;
        border-radius: 8px;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
</style>
""", unsafe_allow_html=True)

# å®šæ•°
SLACK_CHANNEL = "#news-bot1"
CACHE_DIR = "arxiv_cache"

GPT_MODELS = {
    "GPT-4o": "gpt-4o-2024-08-06",
    "GPT-4.1 nano": "gpt-4.1-nano-2025-04-14", 
    "GPT-4.1": "gpt-4.1-2025-04-14",
    "o3": "o3-2025-04-16"
}

DEFAULT_PROMPT = """ã¾ãšã€ä¸ãˆã‚‰ã‚ŒãŸè«–æ–‡ã®èƒŒæ™¯ã¨ãªã£ã¦ã„ãŸèª²é¡Œã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚
æ¬¡ã«ã€è¦ç‚¹ã‚’3ç‚¹ã€ã¾ã¨ã‚ã¦ä¸‹ã•ã„ã€‚
æ›´ã«ã€ä»Šå¾Œã®å±•æœ›ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
æœ€å¾Œã«ã€ä¸ãˆã‚‰ã‚ŒãŸè«–æ–‡ã«ã¤ã„ã¦æƒ³å®šã•ã‚Œå¾—ã‚‹æ‰¹åˆ¤ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‚‰ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
```
ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã®æ—¥æœ¬èªè¨³
ãƒ»èƒŒæ™¯èª²é¡Œ
ãƒ»è¦ç‚¹1
ãƒ»è¦ç‚¹2
ãƒ»è¦ç‚¹3
ãƒ»ä»Šå¾Œã®å±•æœ›
ãƒ»æƒ³å®šã•ã‚Œã‚‹æ‰¹åˆ¤
```
"""

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹
class ArxivCache:
    def __init__(self, cache_dir=CACHE_DIR):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cached_results(self, query, max_age_hours=24):
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        
        if os.path.exists(cache_path):
            mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))
            if datetime.now() - mtime < timedelta(hours=max_age_hours):
                try:
                    with open(cache_path, 'rb') as f:
                        return pickle.load(f)
                except Exception:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ã‚‹å ´åˆã¯å‰Šé™¤
                    os.remove(cache_path)
        
        return None
    
    def save_results(self, query, results):
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(results, f)
        except Exception as e:
            st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã«å¤±æ•—: {e}")

# APIè¨­å®šã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
@st.cache_resource
def initialize_apis():
    """APIåˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    try:
        openai_key = st.secrets.gptApiKey.key
        slack_token = st.secrets.SlackApiKey.key
        notion_key = st.secrets.NotionApiKey.key
        notion_db_url = st.secrets.NotionDatabaseUrl.key
        
        openai.api_key = openai_key
        notion_client = Client(auth=notion_key)
        slack_client = WebClient(token=slack_token)
        
        # æœ€é©åŒ–ã•ã‚ŒãŸarXivã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š
        arxiv_client = arxiv.Client(
            page_size=50,           # å°ã•ã‚ã®ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã§å®‰å®šæ€§å‘ä¸Š
            delay_seconds=3.0,      # arXivæ¨å¥¨ã®3ç§’é–“éš”
            num_retries=5           # å……åˆ†ãªãƒªãƒˆãƒ©ã‚¤å›æ•°
        )
        
        return {
            "openai_key": openai_key,
            "slack_client": slack_client,
            "notion_client": notion_client,
            "notion_db_url": notion_db_url,
            "arxiv_client": arxiv_client,
            "cache": ArxivCache()
        }
    except Exception as e:
        st.error(f"âš ï¸ è¨­å®šã‚¨ãƒ©ãƒ¼: å¿…è¦ãªAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ {e}")
        st.stop()

def extract_arxiv_id_from_url(url):
    """arXiv URLã‹ã‚‰IDã‚’æŠ½å‡ºã™ã‚‹"""
    try:
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å€‹åˆ¥ã«å®šç¾©
        pattern1 = r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5}v?[0-9]*)'
        pattern2 = r'arxiv\.org/pdf/([0-9]{4}\.[0-9]{4,5}v?[0-9]*)\.pdf'
        pattern3 = r'^([0-9]{4}\.[0-9]{4,5}v?[0-9]*)$'
        
        patterns = [pattern1, pattern2, pattern3]
        
        for pattern in patterns:
            match = re.search(pattern, url)
            if match:
                arxiv_id = match.group(1)
                # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å‰Šé™¤
                arxiv_id = re.sub(r'v\d+$', '', arxiv_id)
                
                # IDå½¢å¼ã®æ¤œè¨¼
                validation_pattern = r'^[0-9]{4}\.[0-9]{4,5}$'
                if re.match(validation_pattern, arxiv_id):
                    return arxiv_id
                
        return None
    except Exception:
        return None

def robust_arxiv_search(query, max_results=5, apis=None):
    """å …ç‰¢ãªarXivæ¤œç´¢ï¼ˆ301ã‚¨ãƒ©ãƒ¼å¯¾å¿œï¼‰"""
    if not apis:
        return None
        
    client = apis["arxiv_client"]
    cache = apis["cache"]
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢
    cache_key = f"{query}_{max_results}"
    cached_results = cache.get_cached_results(cache_key, max_age_hours=6)
    if cached_results:
        st.info("ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
        return cached_results
    
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Descending
    )
    
    results = []
    retry_count = 0
    max_retries = 3
    
    while retry_count < max_retries:
        try:
            for result in client.results(search):
                results.append(result)
                if len(results) >= max_results:
                    break
            
            if results:
                # æˆåŠŸã—ãŸå ´åˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                cache.save_results(cache_key, results)
                return results[0] if results else None
            
            break
            
        except arxiv.UnexpectedEmptyPageError as e:
            retry_count += 1
            wait_time = (2 ** retry_count) + random.uniform(0, 1)
            st.warning(f"âš ï¸ arXiv APIã‚¨ãƒ©ãƒ¼ï¼ˆç©ºãƒšãƒ¼ã‚¸ï¼‰ã€‚{wait_time:.1f}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({retry_count}/{max_retries})")
            time.sleep(wait_time)
            
        except arxiv.HTTPError as e:
            if hasattr(e, 'status') and e.status == 301:
                retry_count += 1
                st.warning(f"âš ï¸ HTTP 301ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚¨ãƒ©ãƒ¼ã€‚10ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({retry_count}/{max_retries})")
                time.sleep(10)
            else:
                st.error(f"âŒ HTTP ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        except Exception as e:
            error_msg = str(e)
            if "301" in error_msg:
                retry_count += 1
                st.warning(f"âš ï¸ ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã‚¨ãƒ©ãƒ¼æ¤œå‡ºã€‚{5 * retry_count}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™... ({retry_count}/{max_retries})")
                time.sleep(5 * retry_count)
            else:
                st.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                break
    
    return None

def search_with_semantic_scholar_fallback(query, limit=5):
    """Semantic Scholar APIã‚’ä½¿ã£ãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢"""
    try:
        url = "https://api.semanticscholar.org/graph/v1/paper/search"
        params = {
            'query': f"{query} arxiv",
            'limit': limit,
            'fields': 'title,abstract,authors,venue,year,externalIds,url'
        }
        
        response = requests.get(url, params=params, timeout=10)
        if response.status_code == 200:
            data = response.json().get('data', [])
            
            # arXivè«–æ–‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            arxiv_papers = []
            for paper in data:
                external_ids = paper.get('externalIds', {})
                if external_ids and 'ArXiv' in external_ids:
                    arxiv_id = external_ids['ArXiv']
                    
                    # arXivå½¢å¼ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
                    mock_result = type('MockResult', (), {
                        'title': paper.get('title', ''),
                        'summary': paper.get('abstract', ''),
                        'entry_id': f"http://arxiv.org/abs/{arxiv_id}",
                        'published': datetime.now(),
                        'authors': [type('Author', (), {'name': author.get('name', '')})() 
                                  for author in paper.get('authors', [])]
                    })()
                    
                    arxiv_papers.append(mock_result)
            
            return arxiv_papers[0] if arxiv_papers else None
            
    except Exception as e:
        st.warning(f"âš ï¸ Semantic Scholar APIã‚¨ãƒ©ãƒ¼: {e}")
        return None

def search_paper_by_title(title, apis):
    """ã‚¿ã‚¤ãƒˆãƒ«ã§è«–æ–‡ã‚’æ¤œç´¢ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä»˜ãï¼‰"""
    try:
        # ã¾ãšå®Œå…¨ä¸€è‡´æ¤œç´¢ã‚’è©¦è¡Œ
        query = f'ti:"{title}"'
        result = robust_arxiv_search(query, max_results=3, apis=apis)
        
        if result:
            return result
        
        # å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
        st.info("ğŸ” å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        result = robust_arxiv_search(title, max_results=5, apis=apis)
        
        if result:
            return result
        
        # arXiv APIãŒå¤±æ•—ã—ãŸå ´åˆã€Semantic Scholarã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.warning("âš ï¸ arXivæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Semantic Scholar APIã‚’è©¦è¡Œä¸­...")
        result = search_with_semantic_scholar_fallback(title, limit=5)
        
        if result:
            st.success("âœ… Semantic ScholarçµŒç”±ã§è«–æ–‡ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
            return result
        
        return None
        
    except Exception as e:
        st.error(f"âŒ è«–æ–‡æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def search_paper_by_id(arxiv_id, apis):
    """arXiv IDã§è«–æ–‡ã‚’æ¤œç´¢ï¼ˆå¤šæ®µéšæ¤œç´¢ï¼‰"""
    try:
        # ã¾ãšIDç›´æ¥æ¤œç´¢ã‚’è©¦è¡Œ
        search = arxiv.Search(id_list=[arxiv_id])
        result = robust_arxiv_search(f"id_list:{arxiv_id}", max_results=1, apis=apis)
        
        if result:
            return result
        
        # IDæ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€IDã§ã‚¯ã‚¨ãƒªæ¤œç´¢ã‚’è©¦è¡Œ
        st.info("ğŸ” IDç›´æ¥æ¤œç´¢ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€ã‚¯ã‚¨ãƒªæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        result = robust_arxiv_search(f"id:{arxiv_id}", max_results=1, apis=apis)
        
        if result:
            return result
            
        # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ä¸€èˆ¬æ¤œç´¢ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.info("ğŸ” é€šå¸¸ã®ã‚¯ã‚¨ãƒªæ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        result = robust_arxiv_search(arxiv_id, max_results=5, apis=apis)
        
        if result:
            return result
        
        # Semantic Scholar APIã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        st.warning("âš ï¸ arXivæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸã€‚Semantic Scholar APIã‚’è©¦è¡Œä¸­...")
        result = search_with_semantic_scholar_fallback(arxiv_id, limit=5)
        
        if result:
            st.success("âœ… Semantic ScholarçµŒç”±ã§è«–æ–‡ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ")
            return result
        
        return None
        
    except Exception as e:
        st.error(f"âŒ è«–æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_summary(prompt, result, model, apis):
    """è«–æ–‡è¦ç´„ã‚’ç”Ÿæˆ"""
    if not prompt.strip():
        st.error("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã§ã™ã€‚")
        return None
        
    text = f"title: {result.title}\nbody: {result.summary}"
    
    try:
        # æ–°ã—ã„OpenAI APIå½¢å¼
        client = openai.OpenAI(api_key=apis["openai_key"])
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": text},
            ],
            temperature=0.25,
            max_tokens=2000,
        )
        summary = response.choices[0].message.content
    except Exception as e:
        # å¤ã„APIå½¢å¼ã§ãƒªãƒˆãƒ©ã‚¤
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.25,
                max_tokens=2000,
            )
            summary = response["choices"][0]["message"]["content"]
        except Exception as e2:
            st.error(f"âŒ OpenAI APIã‚¨ãƒ©ãƒ¼: {e2}")
            return None
    
    if not summary:
        st.error("âŒ è¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    title_en = result.title
    date_str = result.published.strftime("%Y-%m-%d %H:%M:%S")
    message = f"ç™ºè¡Œæ—¥: {date_str}\n{result.entry_id}\n{title_en}\n\n{summary}\n"

    return message

def add_summary_to_notion(summary, apis):
    """Notionã«è¦ç´„ã‚’è¿½åŠ """
    try:
        if not all(key in summary for key in ["title", "summary", "url", "date"]):
            return False, "è¦ç´„ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚"
            
        if not summary["title"].strip() or not summary["summary"].strip():
            return False, "ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯è¦ç´„ãŒç©ºã§ã™ã€‚"
            
        apis["notion_client"].pages.create(**{
            "parent": { 
                'database_id': apis["notion_db_url"]
            },
            "properties": {
                "Name": {
                    "title": [
                        {
                            "text": {
                                "content": summary["title"][:100]
                            }
                        }
                    ],
                },
                "Tags": {
                    "multi_select": [
                        {
                            "name": "arXiv"
                        }
                    ]
                },
                "Published": {
                    "date": {
                        "start": summary["date"]
                    }
                },
                "URL": {
                    "url": summary["url"]
                }
            },
            "children": [
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{ 
                            "type": "text", 
                            "text": { 
                                "content": summary["summary"][:2000]
                            } 
                        }]
                    }
                }
            ]
        })
        return True, "æˆåŠŸ"
    except Exception as e:
        return False, f"Notion API ã‚¨ãƒ©ãƒ¼: {e}"

def post_to_slack(message, apis):
    """Slackã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿"""
    try:
        if not message.strip():
            return False, "æŠ•ç¨¿ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™ã€‚"
            
        if len(message) > 4000:
            message = message[:3900] + "\n...(çœç•¥)"
            
        response = apis["slack_client"].chat_postMessage(
            channel=SLACK_CHANNEL,
            text=message
        )
        return True, "æˆåŠŸ"
    except SlackApiError as e:
        return False, f"Slack API ã‚¨ãƒ©ãƒ¼: {e}"
    except Exception as e:
        return False, f"SlackæŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: {e}"

def display_paper_info(result):
    """è«–æ–‡æƒ…å ±ã‚’è¡¨ç¤º"""
    st.markdown('<div class="paper-info-box">', unsafe_allow_html=True)
    st.markdown("### ğŸ“„ è«–æ–‡æƒ…å ±")
    
    st.write(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {result.title}")
    st.write(f"**ç™ºè¡Œæ—¥:** {result.published.strftime('%Y-%m-%d')}")
    st.write(f"**URL:** {result.entry_id}")
    
    try:
        authors = [author.name for author in result.authors if author.name]
        if authors:
            displayed_authors = authors[:10]
            author_text = ", ".join(displayed_authors)
            if len(authors) > 10:
                author_text += f" ä»– {len(authors) - 10} å"
            st.write(f"**è‘—è€…:** {author_text}")
    except Exception:
        st.write("**è‘—è€…:** æƒ…å ±å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    st.markdown('</div>', unsafe_allow_html=True)

def main():
    # åˆæœŸåŒ–
    apis = initialize_apis()
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ“š Paper Summary by ChatGPT</h1>
        <p>arXivã®è«–æ–‡ã‚’æ¤œç´¢ã—ã¦AIã§è¦ç´„ã™ã‚‹ã‚¢ãƒ—ãƒªã§ã™ï¼ˆarXiv API 301ã‚¨ãƒ©ãƒ¼å¯¾å¿œç‰ˆï¼‰</p>
    </div>
    """, unsafe_allow_html=True)

    # APIçŠ¶æ…‹æƒ…å ±ã®è¡¨ç¤º
    with st.expander("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", expanded=False):
        st.markdown("### ğŸ“Š APIå¯¾å¿œçŠ¶æ³")
        st.markdown("- âœ… **arXiv API**: å …ç‰¢ãªãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ã")
        st.markdown("- âœ… **Semantic Scholar**: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œ")
        st.markdown("- âœ… **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**: 6æ™‚é–“æœ‰åŠ¹")
        st.markdown("- âœ… **HTTP 301ã‚¨ãƒ©ãƒ¼å¯¾å¿œ**: è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # GPTãƒ¢ãƒ‡ãƒ«é¸æŠ
        selected_model_name = st.selectbox(
            "GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            options=list(GPT_MODELS.keys()),
            index=0
        )
        selected_model = GPT_MODELS[selected_model_name]
        
        st.info(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: **{selected_model_name}**")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š çµ±è¨ˆæƒ…å ±")
        if 'search_count' not in st.session_state:
            st.session_state.search_count = 0
        if 'error_count' not in st.session_state:
            st.session_state.error_count = 0
        if 'fallback_count' not in st.session_state:
            st.session_state.fallback_count = 0
            
        st.metric("æ¤œç´¢å›æ•°", st.session_state.search_count)
        st.metric("ã‚¨ãƒ©ãƒ¼å›æ•°", st.session_state.error_count)
        st.metric("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆåŠŸ", st.session_state.fallback_count)

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # æ¤œç´¢æ–¹æ³•é¸æŠ
        st.markdown('<div class="search-method-container">', unsafe_allow_html=True)
        search_method = st.radio(
            "è«–æ–‡ã®æŒ‡å®šæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            ["ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢", "URLã¾ãŸã¯IDã§æŒ‡å®š"],
            horizontal=True
        )
        st.markdown('</div>', unsafe_allow_html=True)

        # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if search_method == "ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢":
            paper_input = st.text_input(
                "ğŸ“ arXivã®è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                placeholder="ä¾‹: Attention Is All You Need",
                help="è«–æ–‡ã®æ­£ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚éƒ¨åˆ†ä¸€è‡´ã§ã‚‚æ¤œç´¢ã§ãã¾ã™ã€‚"
            )
        else:
            paper_input = st.text_input(
                "ğŸ”— arXivã®URLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                placeholder="ä¾‹: https://arxiv.org/abs/1706.03762 ã¾ãŸã¯ 1706.03762",
                help="arXivã®URLã€PDFãƒªãƒ³ã‚¯ã€ã¾ãŸã¯IDï¼ˆ1234.5678å½¢å¼ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
            )

    with col2:
        st.markdown("### ğŸ¯ ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹")
        st.markdown("**äººæ°—ã®è«–æ–‡ä¾‹:**")
        
        example_papers = [
            ("Attention Is All You Need", "1706.03762"),
            ("BERT", "1810.04805"),
            ("GPT-3", "2005.14165")
        ]
        
        for title, paper_id in example_papers:
            if st.button(f"ğŸ“„ {title}", key=f"example_{paper_id}"):
                st.session_state.paper_input = paper_id
                st.rerun()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    with st.expander("ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º", expanded=False):
        custom_prompt = st.text_area(
            "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:",
            value=DEFAULT_PROMPT,
            height=300,
            help="è«–æ–‡è¦ç´„ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªç”±ã«ç·¨é›†ã§ãã¾ã™"
        )
    
    # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãŒé–‰ã˜ã¦ã„ã‚‹å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    if 'custom_prompt' not in locals():
        custom_prompt = DEFAULT_PROMPT

    # æ¤œç´¢ãƒœã‚¿ãƒ³
    search_clicked = st.button(
        "ğŸ” è«–æ–‡ã‚’æ¤œç´¢ã—ã¦è¦ç´„", 
        type="primary",
        use_container_width=True
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ã®å…¥åŠ›å¾©å…ƒ
    if 'paper_input' in st.session_state:
        paper_input = st.session_state.paper_input
        del st.session_state.paper_input

    # æ¤œç´¢å®Ÿè¡Œ
    if search_clicked:
        if not paper_input.strip():
            st.error("âŒ è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # æ¤œç´¢ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
        st.session_state.search_count += 1

        # è«–æ–‡æ¤œç´¢
        with st.spinner("ğŸ” è«–æ–‡ã‚’æ¤œç´¢ä¸­ï¼ˆå …ç‰¢ãƒ¢ãƒ¼ãƒ‰ï¼‰..."):
            if search_method == "ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢":
                result = search_paper_by_title(paper_input.strip(), apis)
            else:
                arxiv_id = extract_arxiv_id_from_url(paper_input.strip())
                if not arxiv_id:
                    st.error("âŒ æœ‰åŠ¹ãªarXiv URLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    st.session_state.error_count += 1
                    return
                result = search_paper_by_id(arxiv_id, apis)
            
            if not result:
                st.error("âŒ è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…¥åŠ›å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.session_state.error_count += 1
                return

        # è«–æ–‡æƒ…å ±è¡¨ç¤º
        st.markdown('<div class="success-message">âœ… è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼</div>', unsafe_allow_html=True)
        display_paper_info(result)

        # è¦ç´„ç”Ÿæˆ
        with st.spinner(f"ğŸ¤– {selected_model_name}ã§è¦ç´„ä¸­..."):
            summary_message = get_summary(custom_prompt, result, selected_model, apis)
            
            if not summary_message:
                st.error("âŒ è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.session_state.error_count += 1
                return

            summary_data = {
                "title": result.title,
                "summary": summary_message,
                "url": result.entry_id,
                "date": result.published.strftime("%Y-%m-%d"),
            }

        # çµæœè¡¨ç¤º
        st.markdown("## ğŸ“‹ è¦ç´„çµæœ")
        st.markdown('<div class="summary-box">', unsafe_allow_html=True)
        st.markdown(summary_message)
        st.markdown('</div>', unsafe_allow_html=True)

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“¢ Slackã«æŠ•ç¨¿", use_container_width=True):
                message = "è«–æ–‡ã®ã‚µãƒãƒªã§ã™ã€‚\n" + summary_message
                success, msg = post_to_slack(message, apis)
                if success:
                    st.success("âœ… Slackã«æŠ•ç¨¿ã•ã‚Œã¾ã—ãŸï¼")
                else:
                    st.error(f"âŒ {msg}")

        with col2:
            if st.button("ğŸ“ Notionã«ä¿å­˜", use_container_width=True):
                success, msg = add_summary_to_notion(summary_data, apis)
                if success:
                    st.success("âœ… Notionã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
                else:
                    st.error(f"âŒ {msg}")

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown('<div class="footer-tips">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¡ ä½¿ã„æ–¹ã®ãƒ’ãƒ³ãƒˆ")
    st.markdown("""
    - **arXiv APIå•é¡Œå¯¾å¿œ**: HTTP 301ã‚¨ãƒ©ãƒ¼ã«å¯¾ã™ã‚‹è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ã‚’æ­è¼‰
    - **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½**: arXiv APIãŒå¤±æ•—ã—ãŸå ´åˆã€Semantic Scholar APIã‚’è‡ªå‹•ä½¿ç”¨
    - **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**: åŒã˜æ¤œç´¢ã¯6æ™‚é–“ä»¥å†…ãªã‚‰é«˜é€Ÿè¡¨ç¤º
    - **ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢**: è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚‚å¯èƒ½ï¼‰
    - **URL/IDæŒ‡å®š**: `https://arxiv.org/abs/1234.5678` å½¢å¼ã®URLã¾ãŸã¯ `1234.5678` å½¢å¼ã®IDãŒä½¿ç”¨ã§ãã¾ã™
    - **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: è¦ç´„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã€ã‹ã‚‰ç·¨é›†ã—ã¦ãã ã•ã„
    - **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ç”¨é€”ã«å¿œã˜ã¦GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆo3ãŒæœ€æ–°ã€GPT-4.1ãŒé«˜æ€§èƒ½ã€GPT-4.1 nanoãŒé«˜é€Ÿï¼‰
    """)
    
    st.markdown("### ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„ç‚¹ï¼ˆv12ï¼‰")
    st.markdown("""
    - **301ã‚¨ãƒ©ãƒ¼å¯¾å¿œ**: arXiv APIã®2024å¹´ã‚¤ãƒ³ãƒ•ãƒ©å¤‰æ›´ã«å¯¾å¿œã—ãŸå …ç‰¢ãªãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
    - **Semantic Scholarçµ±åˆ**: arXivæ¤œç´¢å¤±æ•—æ™‚ã®è‡ªå‹•ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    - **æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆè¨­å®š**: 3ç§’é–“éš”ã€5å›ãƒªãƒˆãƒ©ã‚¤ã§å®‰å®šæ€§å‘ä¸Š
    - **ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥**: æ¤œç´¢çµæœã‚’6æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–
    - **è©³ç´°ãªçµ±è¨ˆæƒ…å ±**: æ¤œç´¢æˆåŠŸç‡ã¨ã‚¨ãƒ©ãƒ¼çŠ¶æ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
    """)
    st.markdown('</div>', unsafe_allow_html=True)

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†ï¼ˆ1ç®‡æ‰€ã®ã¿ï¼‰
if __name__ == '__main__':
    main()
