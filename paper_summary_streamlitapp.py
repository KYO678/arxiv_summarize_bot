import streamlit as st
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError
import openai
from notion_client import Client
import re
import time
import random
import requests
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import pickle
import os

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ğŸ“š Paper Summary by ChatGPT",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        margin: -1rem -1rem 2rem -1rem;
        border-radius: 0 0 15px 15px;
    }
    .main-header h1 {
        font-size: 2.5rem;
        margin-bottom: 0.5rem;
    }
    .main-header p {
        font-size: 1.2rem;
        opacity: 0.9;
    }
    .search-method-container {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 10px;
        border: 1px solid #e9ecef;
        margin-bottom: 1rem;
    }
    .paper-info-box {
        background: #e3f2fd;
        border: 1px solid #2196f3;
        border-radius: 10px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    .summary-box {
        background: #f8f9fa;
        border-left: 5px solid #667eea;
        padding: 1.5rem;
        margin: 1rem 0;
        border-radius: 0 10px 10px 0;
    }
    .success-message {
        background: #d4edda;
        color: #155724;
        border: 1px solid #c3e6cb;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .error-message {
        background: #f8d7da;
        color: #721c24;
        border: 1px solid #f5c6cb;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .warning-message {
        background: #fff3cd;
        color: #856404;
        border: 1px solid #ffeaa7;
        border-radius: 8px;
        padding: 1rem;
        margin: 1rem 0;
    }
    .footer-tips {
        background: #f8f9fa;
        border-radius: 10px;
        padding: 1.5rem;
        margin-top: 2rem;
        border: 1px solid #e9ecef;
    }
    .stButton > button {
        width: 100%;
        border-radius: 8px;
        font-weight: 600;
        transition: all 0.3s ease;
    }
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
</style>
""", unsafe_allow_html=True)

# å®šæ•°
SLACK_CHANNEL = "#news-bot1"
CACHE_DIR = "arxiv_cache"
ARXIV_API_BASE = "http://export.arxiv.org/api/query"

GPT_MODELS = {
    "GPT-4o": "gpt-4o-2024-08-06",
    "GPT-4.1 nano": "gpt-4.1-nano-2025-04-14", 
    "GPT-4.1": "gpt-4.1-2025-04-14",
    "o3": "o3-2025-04-16"
}

DEFAULT_PROMPT = """ã¾ãšã€ä¸ãˆã‚‰ã‚ŒãŸè«–æ–‡ã®èƒŒæ™¯ã¨ãªã£ã¦ã„ãŸèª²é¡Œã«ã¤ã„ã¦è¿°ã¹ã¦ãã ã•ã„ã€‚
æ¬¡ã«ã€è¦ç‚¹ã‚’3ç‚¹ã€ã¾ã¨ã‚ã¦ä¸‹ã•ã„ã€‚
æ›´ã«ã€ä»Šå¾Œã®å±•æœ›ã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚
æœ€å¾Œã«ã€ä¸ãˆã‚‰ã‚ŒãŸè«–æ–‡ã«ã¤ã„ã¦æƒ³å®šã•ã‚Œå¾—ã‚‹æ‰¹åˆ¤ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
ã“ã‚Œã‚‰ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
```
ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã®æ—¥æœ¬èªè¨³
ãƒ»èƒŒæ™¯èª²é¡Œ
ãƒ»è¦ç‚¹1
ãƒ»è¦ç‚¹2
ãƒ»è¦ç‚¹3
ãƒ»ä»Šå¾Œã®å±•æœ›
ãƒ»æƒ³å®šã•ã‚Œã‚‹æ‰¹åˆ¤
```
"""

# æ”¹è‰¯ã•ã‚ŒãŸarXivæ¤œç´¢ã‚¯ãƒ©ã‚¹
class ImprovedArxivSearch:
    def __init__(self, cache_dir=CACHE_DIR):
        self.api_base = ARXIV_API_BASE
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def extract_arxiv_id_from_url(self, url):
        """arXiv URLã‹ã‚‰IDã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            patterns = [
                r'arxiv\.org/abs/([0-9]{4}\.[0-9]{4,5}v?[0-9]*)',
                r'arxiv\.org/pdf/([0-9]{4}\.[0-9]{4,5}v?[0-9]*)\.pdf',
                r'^([0-9]{4}\.[0-9]{4,5}v?[0-9]*)$',
                r'arxiv\.org/abs/([a-z-]+/[0-9]{7})',  # å¤ã„å½¢å¼
                r'arxiv\.org/pdf/([a-z-]+/[0-9]{7})'   # å¤ã„å½¢å¼
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    arxiv_id = match.group(1)
                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç•ªå·ã‚’å‰Šé™¤
                    arxiv_id = re.sub(r'v\d+$', '', arxiv_id)
                    return arxiv_id
            
            return None
        except Exception:
            return None
    
    def get_cached_results(self, query, max_age_hours=6):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—"""
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        
        if os.path.exists(cache_path):
            mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))
            if datetime.now() - mtime < timedelta(hours=max_age_hours):
                try:
                    with open(cache_path, 'rb') as f:
                        return pickle.load(f)
                except Exception:
                    os.remove(cache_path)
        
        return None
    
    def save_results(self, query, results):
        """çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜"""
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(results, f)
        except Exception as e:
            st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã«å¤±æ•—: {e}")
    
    def search_arxiv_api(self, query, max_results=5, retry_count=3):
        """ç›´æ¥arXiv APIã§æ¤œç´¢"""
        params = {
            'search_query': query,
            'max_results': max_results,
            'sortBy': 'submittedDate',
            'sortOrder': 'descending'
        }
        
        for attempt in range(retry_count):
            try:
                response = requests.get(self.api_base, params=params, timeout=30)
                response.raise_for_status()
                
                # XMLãƒ‘ãƒ¼ã‚¹
                root = ET.fromstring(response.content)
                
                # åå‰ç©ºé–“ã®è¨­å®š
                namespaces = {
                    'atom': 'http://www.w3.org/2005/Atom',
                    'arxiv': 'http://arxiv.org/schemas/atom'
                }
                
                entries = root.findall('atom:entry', namespaces)
                if not entries:
                    return None
                
                # æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—
                entry = entries[0]
                
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
                paper_data = {
                    'id': entry.find('atom:id', namespaces).text.split('/')[-1],
                    'title': entry.find('atom:title', namespaces).text.strip(),
                    'summary': entry.find('atom:summary', namespaces).text.strip(),
                    'published': entry.find('atom:published', namespaces).text,
                    'updated': entry.find('atom:updated', namespaces).text,
                    'authors': [author.find('atom:name', namespaces).text 
                               for author in entry.findall('atom:author', namespaces)],
                    'categories': [cat.get('term') 
                                  for cat in entry.findall('atom:category', namespaces)]
                }
                
                # URLæƒ…å ±ã‚’è¿½åŠ 
                paper_data['entry_id'] = f"http://arxiv.org/abs/{paper_data['id']}"
                paper_data['pdf_url'] = f"http://arxiv.org/pdf/{paper_data['id']}.pdf"
                
                # æ—¥ä»˜æƒ…å ±ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
                try:
                    paper_data['published_datetime'] = datetime.fromisoformat(paper_data['published'].replace('Z', '+00:00'))
                except:
                    paper_data['published_datetime'] = datetime.now()
                
                return paper_data
                
            except requests.RequestException as e:
                wait_time = (2 ** attempt) + random.uniform(0, 1)
                if attempt < retry_count - 1:
                    st.warning(f"âš ï¸ APIæ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼ˆ{attempt + 1}/{retry_count}ï¼‰ã€‚{wait_time:.1f}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(wait_time)
                else:
                    st.error(f"âŒ APIæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    
            except ET.ParseError as e:
                st.error(f"âŒ XMLãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                break
                
        return None
    
    def search_by_id(self, arxiv_id):
        """IDã§è«–æ–‡ã‚’æ¤œç´¢"""
        cache_key = f"id:{arxiv_id}"
        cached_result = self.get_cached_results(cache_key)
        if cached_result:
            st.info("ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
            return cached_result
        
        # IDæ¤œç´¢ç”¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        params = {
            'id_list': arxiv_id,
            'max_results': 1
        }
        
        try:
            response = requests.get(self.api_base, params=params, timeout=30)
            response.raise_for_status()
            
            # XMLãƒ‘ãƒ¼ã‚¹
            root = ET.fromstring(response.content)
            
            # åå‰ç©ºé–“ã®è¨­å®š
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            entry = root.find('atom:entry', namespaces)
            if entry is None:
                return None
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
            paper_data = {
                'id': entry.find('atom:id', namespaces).text.split('/')[-1],
                'title': entry.find('atom:title', namespaces).text.strip(),
                'summary': entry.find('atom:summary', namespaces).text.strip(),
                'published': entry.find('atom:published', namespaces).text,
                'updated': entry.find('atom:updated', namespaces).text,
                'authors': [author.find('atom:name', namespaces).text 
                           for author in entry.findall('atom:author', namespaces)],
                'categories': [cat.get('term') 
                              for cat in entry.findall('atom:category', namespaces)]
            }
            
            # URLæƒ…å ±ã‚’è¿½åŠ 
            paper_data['entry_id'] = f"http://arxiv.org/abs/{paper_data['id']}"
            paper_data['pdf_url'] = f"http://arxiv.org/pdf/{paper_data['id']}.pdf"
            
            # æ—¥ä»˜æƒ…å ±ã‚’datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            try:
                paper_data['published_datetime'] = datetime.fromisoformat(paper_data['published'].replace('Z', '+00:00'))
            except:
                paper_data['published_datetime'] = datetime.now()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
            self.save_results(cache_key, paper_data)
            
            return paper_data
            
        except Exception as e:
            st.error(f"âŒ IDæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def search_by_title(self, title):
        """ã‚¿ã‚¤ãƒˆãƒ«ã§è«–æ–‡ã‚’æ¤œç´¢"""
        cache_key = f"title:{title}"
        cached_result = self.get_cached_results(cache_key)
        if cached_result:
            st.info("ğŸ—„ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰çµæœã‚’å–å¾—ã—ã¾ã—ãŸ")
            return cached_result
        
        # ã¾ãšå®Œå…¨ä¸€è‡´æ¤œç´¢ã‚’è©¦è¡Œ
        query = f'ti:"{title}"'
        result = self.search_arxiv_api(query, max_results=3)
        
        if result:
            self.save_results(cache_key, result)
            return result
        
        # å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€éƒ¨åˆ†ä¸€è‡´æ¤œç´¢
        st.info("ğŸ” å®Œå…¨ä¸€è‡´ã§è¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸãŸã‚ã€éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...")
        result = self.search_arxiv_api(f"all:{title}", max_results=5)
        
        if result:
            self.save_results(cache_key, result)
            return result
        
        return None

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒ©ã‚¹
class ArxivCache:
    def __init__(self, cache_dir=CACHE_DIR):
        self.cache_dir = cache_dir
        os.makedirs(cache_dir, exist_ok=True)
    
    def get_cached_results(self, query, max_age_hours=24):
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        
        if os.path.exists(cache_path):
            mtime = datetime.fromtimestamp(os.path.getmtime(cache_path))
            if datetime.now() - mtime < timedelta(hours=max_age_hours):
                try:
                    with open(cache_path, 'rb') as f:
                        return pickle.load(f)
                except Exception:
                    os.remove(cache_path)
        
        return None
    
    def save_results(self, query, results):
        cache_path = os.path.join(self.cache_dir, f"{hash(query)}.pkl")
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(results, f)
        except Exception as e:
            st.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã«å¤±æ•—: {e}")

# APIè¨­å®šã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
@st.cache_resource
def initialize_apis():
    """APIåˆæœŸåŒ–ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰"""
    try:
        # API ã‚­ãƒ¼ã®å–å¾—ã¨ãƒ‡ãƒãƒƒã‚°
        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: APIåˆæœŸåŒ–é–‹å§‹")
        
        openai_key = st.secrets.gptApiKey.key
        slack_token = st.secrets.SlackApiKey.key
        notion_key = st.secrets.NotionApiKey.key
        notion_db_url = st.secrets.NotionDatabaseUrl.key
        
        # ã‚­ãƒ¼ã®å­˜åœ¨ç¢ºèªï¼ˆå®Ÿéš›ã®å€¤ã¯è¡¨ç¤ºã—ãªã„ï¼‰
        st.write(f"- OpenAI Key: {'âœ… è¨­å®šæ¸ˆã¿' if openai_key else 'âŒ æœªè¨­å®š'}")
        st.write(f"- Slack Token: {'âœ… è¨­å®šæ¸ˆã¿' if slack_token else 'âŒ æœªè¨­å®š'}")
        st.write(f"- Notion Key: {'âœ… è¨­å®šæ¸ˆã¿' if notion_key else 'âŒ æœªè¨­å®š'}")
        st.write(f"- Notion DB URL: {'âœ… è¨­å®šæ¸ˆã¿' if notion_db_url else 'âŒ æœªè¨­å®š'}")
        
        # OpenAIåˆæœŸåŒ–
        openai.api_key = openai_key
        
        # NotionåˆæœŸåŒ–
        notion_client = Client(auth=notion_key)
        
        # SlackåˆæœŸåŒ–
        slack_client = WebClient(token=slack_token)
        
        # æ”¹è‰¯ã•ã‚ŒãŸarXivæ¤œç´¢ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
        arxiv_search = ImprovedArxivSearch()
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: APIæ¥ç¶šãƒ†ã‚¹ãƒˆ")
        
        # Slackæ¥ç¶šãƒ†ã‚¹ãƒˆ
        try:
            slack_test = slack_client.auth_test()
            st.write(f"- Slack: âœ… æ¥ç¶šæˆåŠŸ (User: {slack_test.get('user', 'Unknown')})")
        except Exception as e:
            st.write(f"- Slack: âŒ æ¥ç¶šå¤±æ•— ({str(e)})")
        
        # Notionæ¥ç¶šãƒ†ã‚¹ãƒˆ
        try:
            notion_test = notion_client.users.me()
            st.write(f"- Notion: âœ… æ¥ç¶šæˆåŠŸ (User: {notion_test.get('name', 'Unknown')})")
        except Exception as e:
            st.write(f"- Notion: âŒ æ¥ç¶šå¤±æ•— ({str(e)})")
        
        return {
            "openai_key": openai_key,
            "slack_client": slack_client,
            "notion_client": notion_client,
            "notion_db_url": notion_db_url,
            "arxiv_search": arxiv_search,
            "cache": ArxivCache()
        }
    except Exception as e:
        st.error(f"âš ï¸ è¨­å®šã‚¨ãƒ©ãƒ¼: å¿…è¦ãªAPIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ {e}")
        st.stop()

def search_paper_by_title(title, apis):
    """ã‚¿ã‚¤ãƒˆãƒ«ã§è«–æ–‡ã‚’æ¤œç´¢"""
    try:
        result = apis["arxiv_search"].search_by_title(title)
        return result
    except Exception as e:
        st.error(f"âŒ è«–æ–‡æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def search_paper_by_id(arxiv_id, apis):
    """arXiv IDã§è«–æ–‡ã‚’æ¤œç´¢"""
    try:
        result = apis["arxiv_search"].search_by_id(arxiv_id)
        return result
    except Exception as e:
        st.error(f"âŒ è«–æ–‡å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_summary(prompt, result, model, apis):
    """è«–æ–‡è¦ç´„ã‚’ç”Ÿæˆ"""
    if not prompt.strip():
        st.error("âŒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒç©ºã§ã™ã€‚")
        return None
        
    text = f"title: {result['title']}\nbody: {result['summary']}"
    
    try:
        # æ–°ã—ã„OpenAI APIå½¢å¼
        client = openai.OpenAI(api_key=apis["openai_key"])
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": text},
            ],
            temperature=0.25,
            max_tokens=2000,
        )
        summary = response.choices[0].message.content
    except Exception as e:
        # å¤ã„APIå½¢å¼ã§ãƒªãƒˆãƒ©ã‚¤
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": text},
                ],
                temperature=0.25,
                max_tokens=2000,
            )
            summary = response["choices"][0]["message"]["content"]
        except Exception as e2:
            st.error(f"âŒ OpenAI APIã‚¨ãƒ©ãƒ¼: {e2}")
            return None
    
    if not summary:
        st.error("âŒ è¦ç´„ãŒç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None
    
    title_en = result['title']
    date_str = result['published_datetime'].strftime("%Y-%m-%d %H:%M:%S")
    message = f"ç™ºè¡Œæ—¥: {date_str}\n{result['entry_id']}\n{title_en}\n\n{summary}\n"

    return message

def add_summary_to_notion(summary, apis):
    """Notionã«è¦ç´„ã‚’è¿½åŠ """
    try:
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: Notioné€£æºé–‹å§‹")
        st.write(f"- summary keys: {list(summary.keys())}")
        st.write(f"- notion_db_url: {apis['notion_db_url']}")
        
        # ãƒ‡ãƒ¼ã‚¿æ¤œè¨¼
        if not all(key in summary for key in ["title", "summary", "url", "date"]):
            missing_keys = [key for key in ["title", "summary", "url", "date"] if key not in summary]
            return False, f"è¦ç´„ãƒ‡ãƒ¼ã‚¿ãŒä¸å®Œå…¨ã§ã™ã€‚ä¸è¶³: {missing_keys}"
            
        if not summary["title"].strip() or not summary["summary"].strip():
            return False, "ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯è¦ç´„ãŒç©ºã§ã™ã€‚"
        
        # Notion Database IDã®å½¢å¼ç¢ºèª
        notion_db_id = apis["notion_db_url"]
        if "notion.so" in notion_db_id:
            # URLã‹ã‚‰IDã‚’æŠ½å‡º
            import re
            match = re.search(r'([a-f0-9]{32})', notion_db_id)
            if match:
                notion_db_id = match.group(1)
            else:
                return False, "Notion Database URLã‹ã‚‰IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ"
        
        # ãƒã‚¤ãƒ•ãƒ³ã‚’é™¤å»
        notion_db_id = notion_db_id.replace('-', '')
        st.write(f"- å‡¦ç†å¾Œã®Database ID: {notion_db_id}")
        
        # Notionãƒšãƒ¼ã‚¸ä½œæˆ
        page_data = {
            "parent": { 
                'database_id': notion_db_id
            },
            "properties": {
                "Name": {
                    "title": [
                        {
                            "text": {
                                "content": summary["title"][:100]
                            }
                        }
                    ],
                },
                "Tags": {
                    "multi_select": [
                        {
                            "name": "arXiv"
                        }
                    ]
                },
                "Published": {
                    "date": {
                        "start": summary["date"]
                    }
                },
                "URL": {
                    "url": summary["url"]
                }
            },
            "children": [
                {
                    "object": "block",
                    "type": "paragraph",
                    "paragraph": {
                        "rich_text": [{ 
                            "type": "text", 
                            "text": { 
                                "content": summary["summary"][:2000]
                            } 
                        }]
                    }
                }
            ]
        }
        
        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: Notion APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        result = apis["notion_client"].pages.create(**page_data)
        st.write(f"âœ… Notion APIãƒ¬ã‚¹ãƒãƒ³ã‚¹: {result.get('id', 'IDä¸æ˜')}")
        
        return True, "æˆåŠŸ"
        
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        st.write(f"âŒ è©³ç´°ã‚¨ãƒ©ãƒ¼:\n```\n{error_details}\n```")
        return False, f"Notion API ã‚¨ãƒ©ãƒ¼: {str(e)}"

def post_to_slack(message, apis):
    """Slackã«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æŠ•ç¨¿"""
    try:
        # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’è¡¨ç¤º
        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: Slacké€£æºé–‹å§‹")
        st.write(f"- Channel: {SLACK_CHANNEL}")
        st.write(f"- Message length: {len(message)} characters")
        
        if not message.strip():
            return False, "æŠ•ç¨¿ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒç©ºã§ã™ã€‚"
        
        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸é•·åˆ¶é™
        if len(message) > 4000:
            message = message[:3900] + "\n...(çœç•¥)"
            st.write("âš ï¸ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒé•·ã™ãã‚‹ãŸã‚çœç•¥ã—ã¾ã—ãŸ")
        
        # Slack APIå‘¼ã³å‡ºã—
        st.write("ğŸ” ãƒ‡ãƒãƒƒã‚°: Slack APIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡ä¸­...")
        response = apis["slack_client"].chat_postMessage(
            channel=SLACK_CHANNEL,
            text=message
        )
        
        st.write(f"âœ… Slack APIãƒ¬ã‚¹ãƒãƒ³ã‚¹: {response.get('ok', False)}")
        if response.get('ok'):
            st.write(f"- Message timestamp: {response.get('ts', 'N/A')}")
        else:
            st.write(f"- Error: {response.get('error', 'Unknown error')}")
        
        return True, "æˆåŠŸ"
        
    except SlackApiError as e:
        st.write(f"âŒ Slack API Error Code: {e.response['error']}")
        return False, f"Slack API ã‚¨ãƒ©ãƒ¼: {e.response['error']}"
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        st.write(f"âŒ è©³ç´°ã‚¨ãƒ©ãƒ¼:\n```\n{error_details}\n```")
        return False, f"SlackæŠ•ç¨¿ã‚¨ãƒ©ãƒ¼: {str(e)}"

def display_paper_info(result):
    """è«–æ–‡æƒ…å ±ã‚’è¡¨ç¤º"""
    st.markdown('<div class="paper-info-box">', unsafe_allow_html=True)
    st.markdown("### ğŸ“„ è«–æ–‡æƒ…å ±")
    
    st.write(f"**ã‚¿ã‚¤ãƒˆãƒ«:** {result['title']}")
    st.write(f"**ç™ºè¡Œæ—¥:** {result['published_datetime'].strftime('%Y-%m-%d')}")
    st.write(f"**URL:** {result['entry_id']}")
    
    try:
        if result['authors']:
            displayed_authors = result['authors'][:10]
            author_text = ", ".join(displayed_authors)
            if len(result['authors']) > 10:
                author_text += f" ä»– {len(result['authors']) - 10} å"
            st.write(f"**è‘—è€…:** {author_text}")
    except Exception:
        st.write("**è‘—è€…:** æƒ…å ±å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    if result.get('categories'):
        st.write(f"**ã‚«ãƒ†ã‚´ãƒª:** {', '.join(result['categories'][:5])}")
    
    st.markdown('</div>', unsafe_allow_html=True)

def main():
    # åˆæœŸåŒ–
    apis = initialize_apis()
    
    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown("""
    <div class="main-header">
        <h1>ğŸ“š Paper Summary by ChatGPT</h1>
        <p>arXivã®è«–æ–‡ã‚’æ¤œç´¢ã—ã¦AIã§è¦ç´„ã™ã‚‹ã‚¢ãƒ—ãƒªã§ã™ï¼ˆç›´æ¥APIå¯¾å¿œç‰ˆï¼‰</p>
    </div>
    """, unsafe_allow_html=True)

    # APIçŠ¶æ…‹æƒ…å ±ã®è¡¨ç¤º
    with st.expander("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", expanded=False):
        st.markdown("### ğŸ“Š APIå¯¾å¿œçŠ¶æ³")
        st.markdown("- âœ… **arXiv API**: ç›´æ¥HTTP APIçµŒç”±ã§å®‰å®šå‹•ä½œ")
        st.markdown("- âœ… **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**: 6æ™‚é–“æœ‰åŠ¹")
        st.markdown("- âœ… **XML ãƒ‘ãƒ¼ã‚¹**: ç¢ºå®Ÿãªãƒ‡ãƒ¼ã‚¿å–å¾—")
        st.markdown("- âœ… **è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤**: æ¥ç¶šã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•å¾©æ—§")

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # GPTãƒ¢ãƒ‡ãƒ«é¸æŠ
        selected_model_name = st.selectbox(
            "GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            options=list(GPT_MODELS.keys()),
            index=0
        )
        selected_model = GPT_MODELS[selected_model_name]
        
        st.info(f"é¸æŠã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«: **{selected_model_name}**")
        
        st.markdown("---")
        st.markdown("### ğŸ“Š çµ±è¨ˆæƒ…å ±")
        if 'search_count' not in st.session_state:
            st.session_state.search_count = 0
        if 'error_count' not in st.session_state:
            st.session_state.error_count = 0
        if 'cache_hits' not in st.session_state:
            st.session_state.cache_hits = 0
            
        st.metric("æ¤œç´¢å›æ•°", st.session_state.search_count)
        st.metric("ã‚¨ãƒ©ãƒ¼å›æ•°", st.session_state.error_count)
        st.metric("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ", st.session_state.cache_hits)

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # æ¤œç´¢æ–¹æ³•é¸æŠ
        st.markdown('<div class="search-method-container">', unsafe_allow_html=True)
        search_method = st.radio(
            "è«–æ–‡ã®æŒ‡å®šæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„:",
            ["ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢", "URLã¾ãŸã¯IDã§æŒ‡å®š"],
            horizontal=True
        )
        st.markdown('</div>', unsafe_allow_html=True)

        # å…¥åŠ›ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰
        if search_method == "ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢":
            paper_input = st.text_input(
                "ğŸ“ arXivã®è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                placeholder="ä¾‹: Attention Is All You Need",
                help="è«–æ–‡ã®æ­£ç¢ºãªã‚¿ã‚¤ãƒˆãƒ«ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚éƒ¨åˆ†ä¸€è‡´ã§ã‚‚æ¤œç´¢ã§ãã¾ã™ã€‚"
            )
        else:
            paper_input = st.text_input(
                "ğŸ”— arXivã®URLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„:",
                placeholder="ä¾‹: https://arxiv.org/abs/1706.03762 ã¾ãŸã¯ 1706.03762",
                help="arXivã®URLã€PDFãƒªãƒ³ã‚¯ã€ã¾ãŸã¯IDï¼ˆ1234.5678å½¢å¼ï¼‰ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚"
            )

    with col2:
        st.markdown("### ğŸ¯ ã‚¯ã‚¤ãƒƒã‚¯ã‚¢ã‚¯ã‚»ã‚¹")
        st.markdown("**äººæ°—ã®è«–æ–‡ä¾‹:**")
        
        example_papers = [
            ("Attention Is All You Need", "1706.03762"),
            ("BERT", "1810.04805"),
            ("GPT-3", "2005.14165")
        ]
        
        for title, paper_id in example_papers:
            if st.button(f"ğŸ“„ {title}", key=f"example_{paper_id}"):
                st.session_state.paper_input = paper_id
                st.rerun()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
    with st.expander("ğŸ”§ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º", expanded=False):
        custom_prompt = st.text_area(
            "ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:",
            value=DEFAULT_PROMPT,
            height=300,
            help="è«–æ–‡è¦ç´„ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªç”±ã«ç·¨é›†ã§ãã¾ã™"
        )
    
    # ã‚¨ã‚­ã‚¹ãƒ‘ãƒ³ãƒ€ãƒ¼ãŒé–‰ã˜ã¦ã„ã‚‹å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨
    if 'custom_prompt' not in locals():
        custom_prompt = DEFAULT_PROMPT

    # æ¤œç´¢ãƒœã‚¿ãƒ³
    search_clicked = st.button(
        "ğŸ” è«–æ–‡ã‚’æ¤œç´¢ã—ã¦è¦ç´„", 
        type="primary",
        use_container_width=True
    )

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‹ã‚‰ã®å…¥åŠ›å¾©å…ƒ
    if 'paper_input' in st.session_state:
        paper_input = st.session_state.paper_input
        del st.session_state.paper_input

    # æ¤œç´¢å®Ÿè¡Œ
    if search_clicked:
        if not paper_input.strip():
            st.error("âŒ è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã¾ãŸã¯URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # æ¤œç´¢ã‚«ã‚¦ãƒ³ãƒˆæ›´æ–°
        st.session_state.search_count += 1

        # è«–æ–‡æ¤œç´¢
        with st.spinner("ğŸ” è«–æ–‡ã‚’æ¤œç´¢ä¸­ï¼ˆç›´æ¥APIçµŒç”±ï¼‰..."):
            if search_method == "ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢":
                result = search_paper_by_title(paper_input.strip(), apis)
            else:
                arxiv_id = apis["arxiv_search"].extract_arxiv_id_from_url(paper_input.strip())
                if not arxiv_id:
                    st.error("âŒ æœ‰åŠ¹ãªarXiv URLã¾ãŸã¯IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
                    st.session_state.error_count += 1
                    return
                result = search_paper_by_id(arxiv_id, apis)
            
            if not result:
                st.error("âŒ è©²å½“ã™ã‚‹è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚å…¥åŠ›å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                st.session_state.error_count += 1
                return

        # è«–æ–‡æƒ…å ±è¡¨ç¤º
        st.markdown('<div class="success-message">âœ… è«–æ–‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼</div>', unsafe_allow_html=True)
        display_paper_info(result)

        # è¦ç´„ç”Ÿæˆ
        with st.spinner(f"ğŸ¤– {selected_model_name}ã§è¦ç´„ä¸­..."):
            summary_message = get_summary(custom_prompt, result, selected_model, apis)
            
            if not summary_message:
                st.error("âŒ è¦ç´„ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                st.session_state.error_count += 1
                return

            summary_data = {
                "title": result['title'],
                "summary": summary_message,
                "url": result['entry_id'],
                "date": result['published_datetime'].strftime("%Y-%m-%d"),
            }

        # çµæœè¡¨ç¤º
        st.markdown("## ğŸ“‹ è¦ç´„çµæœ")
        st.markdown('<div class="summary-box">', unsafe_allow_html=True)
        st.markdown(summary_message)
        st.markdown('</div>', unsafe_allow_html=True)

        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒœã‚¿ãƒ³
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸ“¢ Slackã«æŠ•ç¨¿", use_container_width=True):
                with st.expander("ğŸ” SlackæŠ•ç¨¿ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
                    message = "è«–æ–‡ã®ã‚µãƒãƒªã§ã™ã€‚\n" + summary_message
                    success, msg = post_to_slack(message, apis)
                if success:
                    st.success("âœ… Slackã«æŠ•ç¨¿ã•ã‚Œã¾ã—ãŸï¼")
                else:
                    st.error(f"âŒ {msg}")

        with col2:
            if st.button("ğŸ“ Notionã«ä¿å­˜", use_container_width=True):
                with st.expander("ğŸ” Notionä¿å­˜ãƒ‡ãƒãƒƒã‚°æƒ…å ±", expanded=True):
                    success, msg = add_summary_to_notion(summary_data, apis)
                if success:
                    st.success("âœ… Notionã«ä¿å­˜ã•ã‚Œã¾ã—ãŸï¼")
                else:
                    st.error(f"âŒ {msg}")

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown('<div class="footer-tips">', unsafe_allow_html=True)
    st.markdown("### ğŸ’¡ ä½¿ã„æ–¹ã®ãƒ’ãƒ³ãƒˆ")
    st.markdown("""
    - **ç›´æ¥APIå¯¾å¿œ**: arXivå…¬å¼APIã‚’ç›´æ¥HTTPçµŒç”±ã§å‘¼ã³å‡ºã™ãŸã‚å®‰å®šå‹•ä½œ
    - **XMLãƒ‘ãƒ¼ã‚¹**: ç¢ºå®Ÿãªãƒ‡ãƒ¼ã‚¿å–å¾—ã®ãŸã‚ã®XMLè§£æ
    - **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½**: åŒã˜æ¤œç´¢ã¯6æ™‚é–“ä»¥å†…ãªã‚‰é«˜é€Ÿè¡¨ç¤º
    - **ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢**: è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ­£ç¢ºã«å…¥åŠ›ã—ã¦ãã ã•ã„ï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚‚å¯èƒ½ï¼‰
    - **URL/IDæŒ‡å®š**: `https://arxiv.org/abs/1234.5678` å½¢å¼ã®URLã¾ãŸã¯ `1234.5678` å½¢å¼ã®IDãŒä½¿ç”¨ã§ãã¾ã™
    - **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**: è¦ç´„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€Œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã€ã‹ã‚‰ç·¨é›†ã—ã¦ãã ã•ã„
    - **ãƒ¢ãƒ‡ãƒ«é¸æŠ**: ç”¨é€”ã«å¿œã˜ã¦GPTãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆo3ãŒæœ€æ–°ã€GPT-4.1ãŒé«˜æ€§èƒ½ã€GPT-4.1 nanoãŒé«˜é€Ÿï¼‰
    """)
    
    st.markdown("### ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ æ”¹å–„ç‚¹ï¼ˆv13 - ç›´æ¥APIç‰ˆï¼‰")
    st.markdown("""
    - **ç›´æ¥HTTP API**: arxivãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãšã€arXivå…¬å¼APIã‚’ç›´æ¥å‘¼ã³å‡ºã—
    - **XMLè§£æ**: ElementTreeã«ã‚ˆã‚‹ç¢ºå®Ÿãªãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æ
    - **IDæ¤œç´¢**: id_listãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã‚‹æ­£ç¢ºãªIDæ¤œç´¢
    - **ã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢**: ti:ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«ã‚ˆã‚‹ç²¾å¯†ãªã‚¿ã‚¤ãƒˆãƒ«æ¤œç´¢ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    - **ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€é©åŒ–**: æ¤œç´¢çµæœã‚’6æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦é«˜é€ŸåŒ–
    - **ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°**: æ¥ç¶šã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½
    - **çµ±è¨ˆæƒ…å ±**: æ¤œç´¢æˆåŠŸç‡ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–
    """)
    st.markdown('</div>', unsafe_allow_html=True)

# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†ï¼ˆ1ç®‡æ‰€ã®ã¿ï¼‰
if __name__ == '__main__':
    main()
